# í”Œë¼ìŠ¤í¬ì™€ í¬ë¡¤ë§(ìŠ¤í¬ë˜í¼)ë¥¼ í™œìš©í•œ ì›¹ì‚¬ì´íŠ¸ ì œì‘

<br/>

## ğŸ“Œ Skills
### Language
<a><img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white"/></a>
<a><img src="https://img.shields.io/badge/JavaScript-F7DF1E?style=for-the-badge&logo=JavaScript&logoColor=white"/></a>

### Framework
<a><img src="https://img.shields.io/badge/Flask-000000?style=for-the-badge&logo=flask&logoColor=white"/></a>

### IDE
<a><img src="https://img.shields.io/badge/PyCharm-000000.svg?&style=for-the-badge&logo=PyCharm&logoColor=white"/></a>
<a><img src="https://img.shields.io/badge/Visual_Studio_Code-0078D4?style=for-the-badge&logo=visual%20studio%20code&logoColor=white"/></a>

### Skills
<a><img src="https://img.shields.io/badge/Bootstrap-563D7C?style=for-the-badge&logo=bootstrap&logoColor=white"/><a>
<a><img src="https://img.shields.io/badge/HTML5-E34F26?style=for-the-badge&logo=html5&logoColor=white"/></a>
<a><img src="https://img.shields.io/badge/CSS-239120?&style=for-the-badge&logo=css3&logoColor=white"/></a>
<a><img src="https://img.shields.io/badge/BeautifulSoup-ffffff?&style=for-the-badge&logoColor=black"/></a>
<a><img src="https://img.shields.io/badge/Konlpy-3366FF?&style=for-the-badge&logoColor=black"/></a>
<br/>

<br/>

## ğŸ“Œ Backend Descriptions
### `Search-Box`
> âœï¸ ë„¤ì´ë²„ ê²€ìƒ‰ì°½ì²˜ëŸ¼ ì‚¬ìš©ìì—ê²Œ ê²€ìƒ‰ì„ ë°›ê³ , ì›í•˜ëŠ” ê°œìˆ˜ ë§Œí¼ 'ë„¤ì´ë²„ ë‰´ìŠ¤'ì˜ ë‚´ìš©ì„ BeautifulSoupì„ ì‚¬ìš©í•˜ì—¬ ìŠ¤í¬ë©í•©ë‹ˆë‹¤. <br/>
> 1í˜ì´ì§€ë‹¹ 10ê°œì”©ì´ê¸° ë•Œë¬¸ì—, 91ê°œë¡œ ì„¤ì •ì‹œì— ì´ 10í˜ì´ì§€ë¥¼ ê¸ì–´ ì˜¤ê²Œ ë©ë‹ˆë‹¤. <br/>
> ì–‘ì´ ë§ì•„ì§ˆ ìˆ˜ë¡ ëŒ€ê¸° ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤. <br/>
> 1. request ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìê°€ form íƒœê·¸(í”„ë¡ íŠ¸)ì—ì„œ ê²€ìƒ‰í•œ í‚¤ì›Œë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. 
> 2. contents_cleansing(contents=ë‚´ìš©ë“¤) : ìŠ¤í¬ë© ëœ ë‚´ìš©ì„ ì •ì œí™” í•´ì£¼ëŠ” í•¨ìˆ˜ ì…ë‹ˆë‹¤.
> 3. crawler(maxNum=ìµœëŒ€ê°œìˆ˜ì„¤ì •, query=ì‚¬ìš©ìê°€ê²€ìƒ‰í•œí‚¤ì›Œë“œì¿¼ë¦¬, sort=ì •ë ¬, s_date=ì‹œì‘ë‚ ì§œ, e_date=ì¢…ë£Œë‚ ì§œ) 

<br/>

```python
import urllib.request

inputKeyword = request.args.get('keyword', default="ì•Œë°”íŠ¸ë¡œìŠ¤", type = str)
keyword = urllib.parse.quote(inputKeyword)
...

# ë‚´ìš© ì •ì œí™” í•¨ìˆ˜
def contents_cleansing(contents):
    first_cleansing_contents = re.sub('<dl>.*?</a> </div> </dd> <dd>', '', str(contents)).strip()  # ì•ì— í•„ìš”ì—†ëŠ” ë¶€ë¶„ ì œê±°
    second_cleansing_contents = re.sub('<ul class="relation_lst">.*?</dd>', '', first_cleansing_contents).strip()  # ë’¤ì— í•„ìš”ì—†ëŠ” ë¶€ë¶„ ì œê±° 
    third_cleansing_contents = re.sub('<.+?>', '', second_cleansing_contents).strip()
    contents_text.append(third_cleansing_contents)

# í¬ë¡¤ëŸ¬ í•¨ìˆ˜
def crawler(maxNum, query, sort, s_date, e_date):
    s_from = s_date.replace(".", "")
    e_to = e_date.replace(".", "")
    page = 1
    maxNum_t = (int(maxNum) - 1) * 10 + 1  # 11= 2í˜ì´ì§€ 21=3í˜ì´ì§€ 31=4í˜ì´ì§€  ...81=9í˜ì´ì§€ , 91=10í˜ì´ì§€, 101=11í˜ì´ì§€

    while page <= maxNum_t:
        url = "https://search.naver.com/search.naver?where=news&query=" + query + "&sort=" + sort + "&ds=" + s_date + "&de=" + e_date + "&nso=so%3Ar%2Cp%3Afrom" + s_from + "to" + e_to + "%2Ca%3A&start=" + str(page)

        response = req.get(url)
        html = response.text

        # ë·°í‹°í’€ì†Œí”„ì˜ ì¸ìê°’ ì§€ì •
        soup = BeautifulSoup(html, 'html.parser')

        # <a>íƒœê·¸ì—ì„œ ì œëª©ê³¼ ë§í¬ì£¼ì†Œ ì¶”ì¶œ
        atags = soup.select('.news_tit')
        for atag in atags:
            title_text.append(atag.text)  # ì œëª©
            link_text.append(atag['href'])  # ë§í¬ì£¼ì†Œ

        # ì‹ ë¬¸ì‚¬ ì¶”ì¶œ
        source_lists = soup.select('.info_group > .press')
        for source_list in source_lists:
            source_text.append(source_list.text)  # ì‹ ë¬¸ì‚¬

        # ë‚ ì§œ ì¶”ì¶œ
        date_lists = soup.select('.info_group > span.info')
        for date_list in date_lists:
            # 1ë©´ 3ë‹¨ ê°™ì€ ìœ„ì¹˜ ì œê±°
            if date_list.text.find("ë©´") == -1:
                date_text.append(date_list.text)

        # ë³¸ë¬¸ìš”ì•½ë³¸
        contents_lists = soup.select('.news_dsc')
        for contents_list in contents_lists:
            contents_cleansing(contents_list)  # ë³¸ë¬¸ìš”ì•½ ì •ì œí™”

        # ëª¨ë“  ë¦¬ìŠ¤íŠ¸ ë”•ì…”ë„ˆë¦¬í˜•íƒœë¡œ ì €ì¥
        result = {"date": date_text, "title": title_text, "press": source_text, "contents": contents_text,
                  "link": link_text}

        df = pd.DataFrame(result)
        page += 10

    df1.append(df)

    # ìƒˆë¡œ ë§Œë“¤ íŒŒì¼ì´ë¦„ ì§€ì •
    filename = datetime.datetime.now().strftime('%Y%m%d-%H') + ".csv"
    df.to_csv(filename, encoding='utf-8-sig')

crawler("100", keyword, "0", "datetime.datetime.now()", "20220430" )
...
```

## ğŸ“Œ Frontend Descriptions

---

