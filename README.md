# í”Œë¼ìŠ¤í¬ì™€ í¬ë¡¤ë§(ìŠ¤í¬ë˜í¼)ë¥¼ í™œìš©í•œ ì›¹ì‚¬ì´íŠ¸ ì œì‘

<br/>

## ğŸ“Œ Backend Skills
### Language
<a><img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white"/></a>

### IDE
<a><img src="https://img.shields.io/badge/PyCharm-000000.svg?&style=for-the-badge&logo=PyCharm&logoColor=white"/></a>

### Skills
<a><img src="https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white"/></a>
<a><img src="https://img.shields.io/badge/ScikitLearn-FF9900?style=for-the-badge"/></a>

<br/>

## ğŸ“Œ Backend Descriptions
### `Route`
> âœï¸ í”Œë¼ìŠ¤í¬ì—ì„œ ë¼ìš°íŠ¸ë¥¼ ì„¤ì •í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.
> 1. get_uploaded_data : í”„ë¡ íŠ¸ì—ì„œ ì—‘ì…€ì„ ì—…ë¡œë“œí•˜ë©´ í•´ë‹¹ ë¼ìš°í„°ë¡œ ë³´ë‚´ì§‘ë‹ˆë‹¤. ë°›ì€ ë°ì´í„°ë¥¼ í† ëŒ€ë¡œ ì „ì—­ë³€ìˆ˜ì— ë‹´ì•„ ì €ì¥í•©ë‹ˆë‹¤.
> 2. get_analysis_list : ì—…ë¡œë“œ ëœ íŒŒì¼ì„ í† ëŒ€ë¡œ ë°ì´í„° ë¶„ì„ì„ ì§„í–‰í•œ í›„, ë¶„ì„ ë°ì´í„°ë¥¼ ë¦¬í„´í•©ë‹ˆë‹¤.
> 3. get_lstm_dat : í”„ë¡ íŠ¸ì—ì„œ ì˜ˆìƒ ì¬ê³  ìˆ˜ëŸ‰ì„ ì…ë ¥í•˜ì—¬ ë°›ì•„ì˜¨ ë°ì´í„°ë¥¼ í† ëŒ€ë¡œ LSTMì„ ì§„í–‰í•˜ì—¬ ìµœì ì˜ ì¬ê³ ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì˜ˆì¸¡í•œ ë°ì´í„°ë¥¼ ë¦¬í„´í•©ë‹ˆë‹¤.

```python
import flask
from flask import Flask, request
from flask_cors import CORS
from werkzeug.utils import secure_filename
import PredictInventory
import TotalAnalyize

app = Flask(__name__)
CORS(app)
global uploadedFile

@app.route('/uploadData', methods=['POST'])
def get_uploaded_data():
    f = request.files['file']
    f.save("./uploadFiles/" + secure_filename(f.filename))
    global uploadedFile
    uploadedFile = f.filename
    return 'Uplaod Success!'

@app.route('/getAnalysisList', methods=["POST", "GET"])
def get_analysis_List():
    data = TotalAnalyize.getTotalAnalyizeList(uploadedFile)
    return data

@app.route('/getLSTMData', methods=["POST", "GET"])
def get_LSTM_data():
    inputValues = request.json
    data = PredictInventory.getLSTMData(inputValues, uploadedFile)
    return data

if __name__ == "__main__":
    app.run()
```

---

### `Data Analysis`
> âœï¸ groupby, ë¹ˆë„ë¶„ì„ ë“± ê¸°ì´ˆì ì¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” ê³³ì…ë‹ˆë‹¤. <br/>
> ê¸°ë³¸ì ì¸ ì „ì²˜ë¦¬ ì‘ì—… ì´í›„ì— ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë¶„ì„ëœ ë°ì´í„°ë¥¼ ë¦¬í„´í•©ë‹ˆë‹¤.

```python
def getTotalAnalyizeList(uploadedFile):
  import pandas as pd
  import warnings
  
  warnings.filterwarnings("ignore")
  
  data = pd.read_csv("./uploadFiles/{}".format(uploadedFile))
  
  # rename the columns
  data.rename(columns={'Product_Code': 'ProductCode',
                       'Product_Category': 'ProductCategory',
                       'Order_Demand': 'OrderDemand'}, inplace=True)
  data.head()
  
  # check the null data
  data.isnull().sum()  # Date 11239
  
  # drop the missing values, we can not fill the date so best way drop missing samples
  data.dropna(inplace=True)
  
  # check the null data again
  data.isnull().sum()
  
  # sort the data according to date column
  data.sort_values('Date', ignore_index=True, inplace=True)
  
  # strë¥¼ ìœ„í•´ strìœ¼ë¡œ ì¼ë‹¨ ë³€ê²½
  data['OrderDemand'] = data['OrderDemand'].astype('str')
  
  # there are () int the OrderDemand column and we need to remove them
  data['OrderDemand'] = data['OrderDemand'].str.replace('(', "")
  data['OrderDemand'] = data['OrderDemand'].str.replace(')', "")
  
  # change the dtype as int64
  data['OrderDemand'] = data['OrderDemand'].astype('int64')
  
  # convert the 'Date' column to datetime format
  data['Date'] = pd.to_datetime(data['Date'])
  
  # create Year, Month, Day columns
  data['Year'] = data["Date"].dt.year
  data['Month'] = data["Date"].dt.month
  data['Day'] = data["Date"].dt.day
  
  # [Monthly] Analysis
  temp_data = data.copy()
  temp_data.Month.replace([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],
                          ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],
                          inplace=True)
  ...

  # [ProductCategory] statistical information about OrderDemand
  productCategoryJSON = {
      "index": data["ProductCategory"].value_counts().index.tolist(),
      "count": pd.Series.tolist(data["ProductCategory"].value_counts())
  }
  ...

  # Warehouse Based Analysis
  warehouseBasedDF = data[["OrderDemand", 'Year', 'Warehouse']] \
      .groupby(["Year", "Warehouse"]) \
      .sum().reset_index().sort_values(by=['Warehouse', 'Year'], ascending=False)
  
  warehouseBasedJSON = {
      "Year": warehouseBasedDF['Year'].tolist(),
      "Warehouse": warehouseBasedDF['Warehouse'].tolist(),
      "OrderDemand": warehouseBasedDF['OrderDemand'].tolist()
  }
  ...

  # Total List Return
  totalList = {
      "productCategoryJSON": productCategoryJSON,
      "warehouseJSON": warehouseJSON,
      "productCodeJSON" : productCodeJSON,
      "yearlyJSON": yearlyJSON,
      "monthlyJSON": monthlyJSON,
      "warehouseBasedJSON": warehouseBasedJSON,
      "productCategoryBasedJSON": productCategoryBasedJSON,
      "productCodeBaseJSON": productCodeBaseJSON
  }
  return totalList
```
[â†‘ ì „ì²´ì½”ë“œë³´ê¸°](https://github.com/bbak0105/AI_Project_Back/blob/main/TotalAnalyize.py)

---

### `Stock Prediction`
> âœï¸ í”„ë¡ íŠ¸ì—ì„œ ì‚¬ìš©ìê°€ ë³´ë‚´ì¤€ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ LSTMìœ¼ë¡œ ì ì • ì¬ê³ ëŸ‰ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤. <br/>
> í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ê³¼ê±° 6ê°œì›”ì˜ ë°ì´í„°ë¡œ, í•™ìŠµ ë°ì´í„°ëŠ” ê³¼ê±° 6~18ê°œì›” ì „ 12ê°œì›”ì˜ ë°ì´í„°ë¡œ ì§„í–‰í•©ë‹ˆë‹¤. <br/>
> ë°ì´í„°ì „ì²˜ë¦¬, MinMaxScaler ì •ê·œí™”, LSTM ëª¨ë¸, Adam ì˜µí‹°ë§ˆì´ì € ë° mean_squared_error ì†ì‹¤í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì»´íŒŒì¼ì„ ì§„í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.

```python
# for LSTM model
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout
from collections import Counter
import math

...
# Create new data with only the "OrderDemand" column
orderD = df.filter(["OrderDemand"])

# Convert the dataframe to a np array
orderD_array = orderD.values

# See the train data len
train_close_len = math.ceil(len(orderD_array) - 6)  # ë§ˆì§€ë§‰ 6ê°œì›”ì€ test dataë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ -6 ì ìš©

# Normalize the data
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(orderD_array)

# Create the training dataset
train_data = scaled_data[0: train_close_len, :]

# Create X_train and y_train
X_train = []
y_train = []

for i in range(12, len(train_data) - 6):  # ê³¼ê±° 6~18ê°œì›” ì „ 12ê°œì›”ê°„ì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì •í•˜ê¸° ìœ„í•´ 12 ì ìš©
    X_train.append(train_data[i - 12: i, 0])
    y_train.append(train_data[i + 6, 0])

# make X_train and y_train np array
X_train, y_train = np.array(X_train), np.array(y_train)

# reshape the data
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

# create the testing dataset
test_data = scaled_data[train_close_len - 18:, :]
# ê³¼ê±° 6~18ê°œì›” ì „ 12ê°œì›”ê°„ì˜ ë°ì´í„° 6ì„¸íŠ¸ ë§Œë“¤ê¸° ìœ„í•´ 24ê°œì›”ì¹˜ í…ŒìŠ¤íŠ¸ ë°ì´í„° í•„ìš”(60-(54-18)=24) -18 ì ìš©)

# create X_test
X_test = []
for i in range(12, len(test_data) - 6):  # 12ê°œì›”ì¹˜ì”© 6ê°œ ë°ì´í„° ì„¸íŠ¸ ë§Œë“¤ê¸°
    X_test.append(test_data[i - 12: i, 0])

# convert the test data to a np array and reshape the test data
X_test = np.array(X_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

# change the parameters of first LSTM model and build the Optimized LSTM Model
optimized_model = Sequential()

optimized_model.add(LSTM(512, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], 1)))
...

# compile the model
optimized_model.compile(optimizer="Adam", loss="mean_squared_error", metrics=['mae'])

# train the optimized model
optimized_model.fit(X_train, y_train,
                    batch_size=32,
                    epochs=20,
                    verbose=1)

# Predict with optimized LSTM model
o_predictions = optimized_model.predict(X_test)
o_predictions = scaler.inverse_transform(o_predictions)

# plot the data
train = orderD[:train_close_len]
valid = orderD[train_close_len:]
valid["Predictions"] = o_predictions

# ìˆ˜ìš”ì˜ˆì¸¡ ê²°ê³¼ë°ì´í„°
Demand_M1 = o_predictions[0]
...

D = o_predictions

# ë³€ìˆ˜ ì…ë ¥
beg_inv = inputValues['begInv'] # ê¸°ì´ˆ ì¬ê³ 
min_inv = inputValues['minInv']  # ìµœì†Œ ìœ ì§€í•´ì•¼í•˜ëŠ” ì¬ê³ 
max_inv = inputValues['maxInv'] # ì €ì¥ ê°€ëŠ¥í•œ ìµœëŒ€ ì¬ê³ 
costs = inputValues['costs'] # í–¥í›„ 6ê°œì›”ê°„ ì˜ˆìƒë˜ëŠ” ê°€ê²©

# sense: LpMaximize or LpMinimize(default)
LP = LpProblem(
    name="LP",
    sense=LpMinimize
)

# DEFINE decision variable
# cat: category, "Continuous"(default), "Integer", "Binary"
X1 = LpVariable(name='M1', lowBound=0, upBound=None, cat='Continuous')
...

# OBJECTIVE function
LP.objective = costs[0] * X1 + costs[1] * X2 + costs[2] * X3 + costs[3] * X4 + costs[4] * X5 + costs[5] * X6

# CONSTRAINTS
constraints = [
    beg_inv + X1 - D[0] <= max_inv,
    beg_inv + X1 + X2 - (D[0] + D[1]) <= max_inv,
    beg_inv + X1 + X2 + X3 - (D[0] + D[1] + D[2]) <= max_inv,
    beg_inv + X1 + X2 + X3 + X4 - (D[0] + D[1] + D[2] + D[3]) <= max_inv,
    beg_inv + X1 + X2 + X3 + X4 + X5 - (D[0] + D[1] + D[2] + D[3] + D[4]) <= max_inv,
    beg_inv + X1 + X2 + X3 + X4 + X5 + X6 - (D[0] + D[1] + D[2] + D[3] + D[4] + D[5]) <= max_inv,
    beg_inv + X1 - D[0] >= min_inv,
    beg_inv + X1 + X2 - (D[0] + D[1]) >= min_inv,
    beg_inv + X1 + X2 + X3 - (D[0] + D[1] + D[2]) >= min_inv,
    beg_inv + X1 + X2 + X3 + X4 - (D[0] + D[1] + D[2] + D[3]) >= min_inv,
    beg_inv + X1 + X2 + X3 + X4 + X5 - (D[0] + D[1] + D[2] + D[3] + D[4]) >= min_inv,
    beg_inv + X1 + X2 + X3 + X4 + X5 + X6 - (D[0] + D[1] + D[2] + D[3] + D[4] + D[5]) >= min_inv
]

for i, c in enumerate(constraints):
    constraint_name = f"const_{i}"
    LP.constraints[constraint_name] = c

# SOLVE model
res = LP.solve()

# ìµœì†Œë¹„ìš© ë„ì¶œ ë° í•„ìš” ì£¼ë¬¸ëŸ‰ ê²°ê³¼
Order_M1 = X1.varValue
...
Min_TotalCost = value(LP.objective)

targetVariables = []
for v in LP.variables():
    targetVariables.append({str(v): v.varValue})
targetVariables.append({"target": str(Min_TotalCost)})

return targetVariables
```

[â†‘ ì „ì²´ì½”ë“œë³´ê¸°](https://github.com/bbak0105/AI_Project_Back/blob/main/PredictInventory.py)
